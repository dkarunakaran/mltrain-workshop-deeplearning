{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Senses via Dictionary Bootstrapping\n",
    "======================\n",
    "This document outlines the pipeline needed to reproduce the results in the UAI paper \"Embedding Senses via Dictionary Bootstrapping\". We only present the minimally required code in order to keep everything as simple as possible. Also, some of the approaches have been slightly modified to illustrate how various techniques can be implemented. For example, the paper uses a sum of squared loss objective for training, while this tutorial uses a hinge loss. The differences are subtle, and you should feel free to test various models of your choice.\n",
    "\n",
    "Prerequisite\n",
    "-----------\n",
    "Before we start, we require the following environmental setups.\n",
    "* Python 2.7 or higher\n",
    "* [Theano 0.9.0](http://deeplearning.net/software/theano/index.html)\n",
    " * In order to use your GPU, you should also set the `floatX` and `device` attributes in your `.theanorc` file\n",
    "* [Matplotlib](https://matplotlib.org/)\n",
    "* [scikit-learn](http://scikit-learn.org/stable/)\n",
    "* CUDA installation (paths to `libcuda` and `nvcc` should be included in `LD_LIBRARY_PATH` and `PATH`, respectively)\n",
    "\n",
    "Introduction\n",
    "------------\n",
    "First, we'll import a number of packages we need in order to build our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import cPickle as pk\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use the following links to download the data. The first link is to the main data file, and the second link is to\n",
    "a collection of various pre-trained models, which we'll show how to use later.\n",
    "* [Data](https://drive.google.com/open?id=0B7V13DJYRnxtdEY5dnc1QnREQUE) (Pickled, 724MB) \n",
    "* [Pre-trained model 1](https://drive.google.com/open?id=0B7V13DJYRnxtVGtqcGpReks4bTQ), [model 2](https://drive.google.com/open?id=0B7V13DJYRnxtTFZZOW12M0FqbEk), [model 3](https://drive.google.com/open?id=0B7V13DJYRnxtNUcweEZvNzBlRFk) (Pickled, 96MB each, trained with $\\beta=0.3,0.4,0.5$)\n",
    "* [Dictionary file](https://drive.google.com/open?id=0B7V13DJYRnxtUzVIZmRzSU5Edm8) (Text, 9.6MB)\n",
    "\n",
    "You won't be needing the dictionary file for this session, since all necessary information have been encoded into the data file. We provide it just for your reference.  \n",
    "After downloading the necessary files, modify the variables `data_in` and `saved_model` below to reflect the new location.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_in = './simple-data.pkl' # Path to input data. \n",
    "model_out = './models/'       # Path to save your model\n",
    "save_name = 'my-save.pkl'     # Name of the file to save your model into\n",
    "saved_model = 'model_example.pkl'       # Path to pre-existing trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the data structure\n",
    "----------------------------\n",
    "Before we proceed with our model, let's take a look at the structure and encoding of the data. This may seem a bit tedious, but it's important for understanding the composition of the model we are about to build. If you're already familiar with the data format, or want to get straight to the point, feel free to skip this section (but the loading of the data is required to continue). \n",
    "\n",
    "We begin by loading the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(data_in, 'rb') as f:\n",
    "    d = pk.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `d` is stored as a string->data hash table, whose keys are as follows:  \n",
    "(PS: the correct term would be 'python dictionary', but we'll use 'hash table' in order to avoid confusion)\n",
    "```\n",
    "'def', 'def_plain', 'dmask', 'wmask', 'idf', 'dw2id', 'aw2id', 'id2dw', 'id2aw'\n",
    "```\n",
    "The main data is the accessed by the `def` key: The `i`-th entry into `d['def']` holds the definition encoding of the `i`-th word in the dictionary.  \n",
    "Then how do we know which word is the `i`-th word? That's why we have a bunch of keys in the form of '(something)2(something)'.  \n",
    "Here, '(something)' can be one of `aw`, `dw`, or `id`, where they stand for 'ambiguous word', 'disambiguous word', and 'id' (duh), respectively.  \n",
    "* Ambiguous words are regular plain words we see every day: `apple`, `boat`, etc.   \n",
    "* Disambiguous words are dictionary entries with their precise sense specified: `apple.n.01`, `boat.n.02`, etc.  \n",
    "* Each (dis)ambiguous word is associated with a unique `id` that can be accessed by the appropriate '(something)2(something)' etnry.  \n",
    "\n",
    "Thus, to get the encoding for the definition of `apple.n.01`, we can use the following piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33785 33786 33787 ...,    -1    -1    -1]\n",
      " [81959 81960 81961 ...,    -1    -1    -1]\n",
      " [13759 63054 63055 ...,    -1    -1    -1]\n",
      " ..., \n",
      " [   -1    -1    -1 ...,    -1    -1    -1]\n",
      " [   -1    -1    -1 ...,    -1    -1    -1]\n",
      " [   -1    -1    -1 ...,    -1    -1    -1]]\n",
      "[1517   49   79   27  677   27 1602  585   42 3200    6 8837 8838  657 3103\n",
      "   59   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1\n",
      "   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1\n",
      "   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1   -1\n",
      "   -1   -1   -1   -1   -1   -1   -1   -1]\n"
     ]
    }
   ],
   "source": [
    "word_id = d['dw2id']['apple.n.01']\n",
    "print(d['def'][word_id])  # show the sense indices for each plain word in the definition (to be accessed by id2dw)\n",
    "print(d['def_plain'][word_id]) # show the plain word indices in the definition (to be accessed by id2aw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the encoding of the definition of `apple.n.01` takes a form of a 2D array, whose size is the maximum length of the definition times the maximum number of senses.  \n",
    "Therefore, the total size of `d['def']` is `num_dw`-by-`num_max_def_len`-by-`num_max_sense`.   \n",
    "If the definition is shorter than `num_max_def_len`, or each ambiguous word has fewer than the `num_max_sense`, the remaining positions will be filled with arbitrary placeholders (-1's, in this case).  \n",
    "The binary mask variables exist to zero-out these placeholders. In particular, `d['dmask']` is used to mask out on the sense level, while `d['wmask']` is used on the word level. These variables will come in handy when we're building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82831, 68, 25)\n",
      "(82831, 68)\n"
     ]
    }
   ],
   "source": [
    "#num_dw, num_max_def_len, num_max_sense = d['def'].shape\n",
    "assert(d['def'].shape == d['dmask'].shape)\n",
    "print(d['dmask'].shape)\n",
    "print(d['wmask'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-negative entries that comprise `d['def']` are indexes into `d['id2dw']`.  \n",
    "For example, let's look at all the possible senses of the third word in the definition of `apple.n.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fruit with red or yellow or green skin and sweet to tart crisp whitish flesh <eos>\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([d['id2aw'][x] for x in d['def_plain'][word_id] if x > 0])) # plain definition of apple.n.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red.s.01', 'red.n.01', 'red.n.02', 'bolshevik.n.01', 'loss.n.06', 'crimson.s.02', 'crimson.s.03']\n"
     ]
    }
   ],
   "source": [
    "print([d['id2dw'][x] for x in d['def'][word_id][2] if x > 0]) # disambiguations of 'red'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model\n",
    "------------------\n",
    "Okay, now that you know how the dictionary is encoded, let's dive into the good part. The first step is to initialize the necessary variables and load them onto the GPU. In addition to the variable initialization, we also provide some helper functions that will become useful in the main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shared_var(n, init_dat=None, shape=None, dtype='int32'):\n",
    "    if init_dat is not None:\n",
    "        value = init_dat\n",
    "    elif shape is not None:\n",
    "        value = nr.uniform(-0.1, 0.1, shape)\n",
    "    else:\n",
    "        return None\n",
    "    return theano.shared(value.astype(dtype), name=n, borrow=True)\n",
    "\n",
    "dtype = theano.config.floatX\n",
    "bs = 64             # minibatch size\n",
    "td = 300 # embedding dimension\n",
    "hd = 600 # hidden dimension\n",
    "nw, mw, ms = d['def'].shape # total number of words, max num. of words per definition, max num. of senses per word\n",
    "params = {} # hash to hold the trainable parameters\n",
    "tau = 10\n",
    "# We need the following 3 lines to account for the discrepancy between \n",
    "# the true number of words (true_nw) vs. the number of words w/ IDs (nw). \n",
    "# This discrepancy exists because of some designs choices that have not been modified.\n",
    "true_nw = nw\n",
    "maxid = np.max(d['def'])\n",
    "if maxid >= nw: nw = maxid + 1\n",
    "\n",
    "#------------------- Shared variables ---------------------------------#\n",
    "dat = shared_var('def', init_dat = d['def'])\n",
    "#pd = shared_var('pd', init_dat = d['def_plain'])\n",
    "dmask = shared_var('dmask', init_dat = d['dmask'], dtype=dtype)\n",
    "wmask = shared_var('wmask', init_dat = d['wmask'], dtype=dtype)\n",
    "sprior = shared_var('sprior', init_dat = np.ones(d['def'].shape), dtype=dtype) # uniform prior over senses\n",
    "h_d = shared_var('idf', init_dat = d['idf'], dtype=dtype)\n",
    "params['dwe'] = shared_var('dwe', shape = (nw, td), dtype = dtype) # disambiguated word embedding\n",
    "params['L'] = shared_var('L', shape=(td, ), dtype=dtype) # diagonal entries only (of the td x td matrix)\n",
    "params['L1'] = shared_var('L1', shape=(td, hd), dtype=dtype)\n",
    "params['L2'] = shared_var('L2', shape = (hd, td), dtype = dtype)\n",
    "\n",
    "## Helper function for saving the model\n",
    "def save_model(save_dat, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pk.dump(save_dat, f, protocol=pk.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before we begin coding, recall how the embedding of each word $v_i$ is computed:\n",
    "$$\n",
    "v_i\\gets(1-\\beta)v_i+\\beta\\left(\\sum_{d\\in d(v_i)}h_d\\sum_{v_j\\in s(d)}\\alpha_{ji}v_j\\right)\n",
    "$$\n",
    "The paper refers to this update as a \"message-passing RNN\" or \"fixed-point update\". Since we don't explicitly deal with RNNs in the traditional sense, this tutorial will refer to it as the fixed-point update.  \n",
    "Here, $d(v_i)$ is the set of all plain words that comprise the definition of $v_i$, and $s(d)$ is the set of all possible senses of a plain word $d$. The update equation states that the new embedding of $v_i$ is computed as a doubly-convex combination of the words that constitute its definition. The hyper-parameter $\\beta\\in(0,1)$ is gradually decreased over training iteration in order to ensure convergence.  \n",
    "The coefficients $h_d$'s are constants set to the inverse document frequency (IDF) of the plain word $d$. These values are pre-computed into the theano variable `idf`.  \n",
    "The second set of coefficients $\\alpha$ are computed as $\\alpha_{ji}\\propto\\Pr(v_j)\\Pr(d_{-i}|v_j)$,\n",
    "where $d_{-i}$ denotes the list of plain words in the definition *except* the `i`-th word; i.e., the context of $d_i$. \n",
    "The values of the prior $\\Pr(v_j)$ is stored in the theano variable `sprior`, while the likelihood of the context will take another trainable form.\n",
    "$$\n",
    "\\Pr(d_{-i}|v_j)=\\prod_{d_m\\in d_{-i}}\\Pr(d_m|v_j)\\propto\\prod_{d_m\\in d_{-i}}\\exp\\left(\\frac{\\tau}{\\left\\vert s(d_m)\\right\\vert}\\sum_{v_k\\in s(d_m)}v_j^TUv_k\\right)\n",
    "$$\n",
    "The parameter $U=LL$ is a trainable positive-definite matrix represented by the theano variable `L`, and $\\tau$ is a hyper-parameter that specifies the 'temperature' of the softmax distribution. Note that the term $v_j^TUv_k$ can be replaced by any function that returns the similarity between $v_j$ and $v_k$.\n",
    "\n",
    "First, we'll write a helper function we call to compute the coefficients $\\alpha_{ji}$'s. This function `to_weight()` returns the $\\alpha$ coefficient for a single dictionary entry. Since we wish to proceed the training in minibatches, the $\\alpha$ computation should be carried over multiple instances. To do this, we `scan()` through the minibatch to collect the results. Note that it is possible to perform this operation purely via matrix operations (as opposed to looping), but that usually requires too much memory and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dwe = params['dwe'] # nw x td\n",
    "L = params['L']     # td\n",
    "L1 = params['L1']   # td x hd\n",
    "L2 = params['L2']   # hd x td\n",
    "\n",
    "wi = T.ivector('wi') # bs\n",
    "df = dat[wi, :]      # bs x mw x ms\n",
    "pr = sprior[wi, :]   # bs x mw x ms\n",
    "dm = dmask[wi, :]    # bs x mw x ms\n",
    "wm = wmask[wi, :].dimshuffle(0, 1, 'x') # bs x mw x 1\n",
    "lr = T.dscalar('lr')\n",
    "idf = h_d[wi].dimshuffle(0, 1, 'x') # bs x mw x 1\n",
    "\n",
    "'''\n",
    "Returns: alpha coefficients of size mw x ms. \n",
    "d: indices of the senses that comprise the definition (mw x ms)\n",
    "m: mask of size mw x ms\n",
    "prior: Pr(v_j) for each v_j in each plain word in the definition (mw x ms)\n",
    "'''\n",
    "def to_weight(d, m, prior):\n",
    "    half_dot = dwe[d, :] * L.dimshuffle('x', 'x', 0) # mw x ms x td\n",
    "    logit = T.tensordot(half_dot, half_dot.T, axes=1) # mw x ms x ms x mw\n",
    "    logit = logit.dimshuffle(0, 1, 3, 2) # mw x ms x mw x ms\n",
    "    cnt = T.sum(m, axis=1).dimshuffle('x', 'x', 0) # 1 x 1 x mw\n",
    "    logit = T.sum(logit * m.dimshuffle('x', 'x', 0, 1), axis=3) / cnt # mw x ms x mw\n",
    "    logit = T.exp(tau * T.switch(T.isnan(logit), 0, logit)) # mw x ms x mw\n",
    "    logit = T.prod(logit, axis=2) * prior # mw x ms\n",
    "    sm = T.sum(logit * m, axis=1, keepdims=True) # mw x 1\n",
    "    logit = (logit * m) / sm # mw x ms\n",
    "    return T.switch(T.or_(T.isnan(logit), T.isinf(logit)), 0, logit)\n",
    "\n",
    "alphas, _ = theano.scan(fn = to_weight, sequences = [df, dm, pr])\n",
    "raw_emb = T.sum(alphas.dimshuffle(0, 1, 2, 'x') * dwe[df, :], axis = 2) # bs x mw x td\n",
    "e_i = T.sum((raw_emb.astype(dtype) * idf) * wm, axis = 1) # bs x td (the doubly-convex combination of senses and plain words)\n",
    "new_emb = T.tanh(T.dot(e_i, L1)) # bs x hd\n",
    "new_emb = T.dot(new_emb, L2) # bs x td, after passing through a 2-layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final computation of `new_emb` is done by summing up the sense-level embeddings, and passing through a two-layer network.\n",
    "$$\n",
    "f_{\\Theta}(e_i)=L_2^T\\tanh(L_1^Te_i), ~~~\\Theta\\triangleq\\{L, L_1, L_2\\}\n",
    "$$\n",
    "Next, we need a training signal to adjust the parameters $\\Theta$. The training criterion we use is how close $f_\\Theta(e_i)$ (i.e., `new_emb`) is to the current vector $u_i$ (i.e., `dwe[wi, :]`). Our paper suggested an L2 regression approach, but here we'll try a hinge loss approach. The hinge loss takes both a positive sample ($u_i$) and a negative sample ($n_i$) and attempts to widen the gap between the two. The negative sample $n_i$ is an embedding of a randomly sampled word not equal to $u_i$.\n",
    "$$\n",
    "\\min_\\Theta\\frac{1}{N}\\sum_i^N\\max\\{0,m-cosim(f_\\Theta(e_i),u_i)+cosim(f_\\Theta(e_i),n_i)\\}\n",
    "$$\n",
    "In words, we are maximizing the average cosine similarities between the positive samples while minimizing the similarities between the negative samples. The differences are mitigated by the margin parameter `m`, which we set to 0.01.\n",
    "\n",
    "We optimize the above objective using a simple stochastic gradient descent (SGD) procedure. Applying more sophisticated optimization algorithms such as Adam, AdaGrad, and RMSProp is straightforward since they are readily available in most DL libraries. Here, we wish to demonstrate the simplest form to show the feasibility of our approach.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nwi = T.ivector('nwi') # indices of negative samples\n",
    "beta = T.dscalar('beta')\n",
    "ndw = dwe[nwi, :] # bs x td\n",
    "pdw = dwe[wi, :]  # bs x td\n",
    "def cosim(x, y):\n",
    "    return T.mean(T.sum(x * y, axis=1) / (x.norm(2, axis=1) * y.norm(2, axis=1)))\n",
    "\n",
    "pos_cost = -cosim(new_emb, pdw) #T.mean(T.sum((new_emb - pdw) ** 2, axis=1)) #\n",
    "neg_cost = -cosim(new_emb, ndw) #T.mean(T.sum((new_emb - ndw) ** 2, axis=1))\n",
    "tot_cost = T.mean(T.maximum(0, 0.01 + pos_cost - neg_cost))\n",
    "## Uncomment the following if you wish to add regularization\n",
    "# tot_cost += 0.1 * T.sum(abs(L1)) + 0.1 * T.sum(abs(L2))\n",
    "all_params = [params['L'], params['L1'], params['L2']] # list of parameters we wish to optimize via SGD\n",
    "\n",
    "## Now we build the SGD-based update\n",
    "grads = T.grad(tot_cost, all_params)\n",
    "updates = [] # updates for the embedding parameters to be used by SGD\n",
    "for (p, g) in zip(all_params, grads):\n",
    "    updates.append((p, p - lr * g))\n",
    "\n",
    "## Next we construct the fixed-point update (or as the paper says, the \"message-passing update\")\n",
    "#fp_term = T.sum(raw_emb * idf.dimshuffle(0, 1, 'x'), axis = 1) # bs x td\n",
    "fp_emb = (1 - beta) * pdw + beta * e_i\n",
    "fp_update = T.set_subtensor(pdw, fp_emb.astype(dtype))\n",
    "dwe_update = [(dwe, fp_update)] # we wish to update only a portion of dwe\n",
    "dwe_diff = T.max(T.abs_(fp_emb - pdw)) # maximum increment\n",
    "\n",
    "train_step = theano.function([wi, nwi, lr], tot_cost, updates = updates) # \\Theta trainer\n",
    "dwe_update_step = theano.function([wi, nwi, beta], [tot_cost, dwe_diff, new_emb], updates = dwe_update)  # fixed-point iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting thing to note here, is that the parameters $L_1, L_2$ are *not* used to compute the actual embedding of $v_i$. Rather, they seem to act as providing the value of $L$ with a rough guidance signal so that $L$ has a better chance of finding good values. This was not intended in design time, as we started off with the following update equation:\n",
    "> fp_emb = (1 - beta) * pdw + beta * new_emb  \n",
    "\n",
    "But that ended up giving not-so-good results. We then substituted `new_emb` with `e_i`, which depends only on $L$, and began getting reasonable embeddings. Feel free to use the above original equation and let us know if you find something interesting (maybe we made a mistake).\n",
    "\n",
    "Training the model\n",
    "--------------------\n",
    "Once we have the necessary theano functions compiled, we are ready to code the main process that will drive the overall training. The basic idea is to alternate between training the $L, L_1, L_2$ parameters via SGD and updating the actual embedding via the fixed-point iteration.\n",
    "We will perform multiple iterations of SGD followed by a single iteration of fixed-point updates. The number of SGD iterations can be adjusted by setting the value of `num_consec_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "[fp] 0, cumul. cost = 0.011, max_diff = 0.032 (1.7 sec.)\n",
      "[fp] 100, cumul. cost = 1.142, max_diff = 0.051 (174.1 sec.)\n",
      "[fp] 200, cumul. cost = 2.073, max_diff = 0.059 (176.2 sec.)\n",
      "[fp] 300, cumul. cost = 3.233, max_diff = 0.073 (171.0 sec.)\n",
      "[fp] 400, cumul. cost = 4.250, max_diff = 0.073 (171.4 sec.)\n",
      "[fp] 500, cumul. cost = 5.418, max_diff = 0.073 (174.1 sec.)\n",
      "[fp] 600, cumul. cost = 6.378, max_diff = 0.073 (180.6 sec.)\n",
      "[fp] 700, cumul. cost = 7.331, max_diff = 0.073 (183.1 sec.)\n",
      "[fp] 800, cumul. cost = 8.351, max_diff = 0.073 (184.6 sec.)\n",
      "[fp] 900, cumul. cost = 9.446, max_diff = 0.073 (184.0 sec.)\n",
      "[fp] 1000, cumul. cost = 10.373, max_diff = 0.073 (178.1 sec.)\n",
      "[fp] 1100, cumul. cost = 11.387, max_diff = 0.073 (176.8 sec.)\n",
      "[fp] 1200, cumul. cost = 12.475, max_diff = 0.073 (175.7 sec.)\n",
      "\t*** Epoch 0, cost = 13.5557944103 ***\n",
      "[fp] 0, cumul. cost = 0.023, max_diff = 0.000 (1.7 sec.)\n",
      "[fp] 100, cumul. cost = 1.163, max_diff = 0.000 (178.2 sec.)\n",
      "[fp] 200, cumul. cost = 2.300, max_diff = 0.000 (181.0 sec.)\n",
      "[fp] 300, cumul. cost = 3.373, max_diff = 0.000 (177.8 sec.)\n",
      "[fp] 400, cumul. cost = 4.432, max_diff = 0.000 (176.1 sec.)\n",
      "[fp] 500, cumul. cost = 5.425, max_diff = 0.000 (165.2 sec.)\n",
      "[fp] 600, cumul. cost = 6.355, max_diff = 0.000 (168.1 sec.)\n",
      "[fp] 700, cumul. cost = 7.451, max_diff = 0.000 (173.8 sec.)\n",
      "[fp] 800, cumul. cost = 8.459, max_diff = 0.000 (180.1 sec.)\n",
      "[fp] 900, cumul. cost = 9.519, max_diff = 0.000 (179.8 sec.)\n",
      "[fp] 1000, cumul. cost = 10.670, max_diff = 0.000 (165.4 sec.)\n",
      "[fp] 1100, cumul. cost = 11.855, max_diff = 0.000 (172.5 sec.)\n",
      "[fp] 1200, cumul. cost = 12.976, max_diff = 0.000 (175.0 sec.)\n",
      "\t*** Epoch 1, cost = 14.0021469591 ***\n",
      "[sgd] 0, cumul. cost = 0.020, max_diff = -inf (5.6 sec.)\n",
      "[sgd] 100, cumul. cost = 0.421, max_diff = -inf (591.4 sec.)\n",
      "[sgd] 200, cumul. cost = 0.521, max_diff = -inf (600.5 sec.)\n",
      "[sgd] 300, cumul. cost = 0.565, max_diff = -inf (604.8 sec.)\n",
      "[sgd] 400, cumul. cost = 0.592, max_diff = -inf (592.0 sec.)\n",
      "[sgd] 500, cumul. cost = 0.627, max_diff = -inf (600.4 sec.)\n",
      "[sgd] 600, cumul. cost = 0.635, max_diff = -inf (596.3 sec.)\n",
      "[sgd] 700, cumul. cost = 0.644, max_diff = -inf (589.2 sec.)\n",
      "[sgd] 800, cumul. cost = 0.651, max_diff = -inf (583.0 sec.)\n",
      "[sgd] 900, cumul. cost = 0.660, max_diff = -inf (589.9 sec.)\n",
      "[sgd] 1000, cumul. cost = 0.661, max_diff = -inf (607.1 sec.)\n",
      "[sgd] 1100, cumul. cost = 0.666, max_diff = -inf (598.0 sec.)\n",
      "[sgd] 1200, cumul. cost = 0.669, max_diff = -inf (596.2 sec.)\n",
      "\t*** Epoch 2, cost = 0.679570349717 ***\n",
      "[sgd] 0, cumul. cost = 0.000, max_diff = -inf (6.1 sec.)\n",
      "[sgd] 100, cumul. cost = 0.002, max_diff = -inf (602.6 sec.)\n",
      "[sgd] 200, cumul. cost = 0.002, max_diff = -inf (612.1 sec.)\n",
      "[sgd] 300, cumul. cost = 0.003, max_diff = -inf (616.0 sec.)\n",
      "[sgd] 400, cumul. cost = 0.005, max_diff = -inf (606.5 sec.)\n",
      "[sgd] 500, cumul. cost = 0.014, max_diff = -inf (577.0 sec.)\n",
      "[sgd] 600, cumul. cost = 0.024, max_diff = -inf (582.7 sec.)\n",
      "[sgd] 700, cumul. cost = 0.030, max_diff = -inf (619.4 sec.)\n",
      "[sgd] 800, cumul. cost = 0.034, max_diff = -inf (600.8 sec.)\n",
      "[sgd] 900, cumul. cost = 0.039, max_diff = -inf (589.8 sec.)\n",
      "[sgd] 1000, cumul. cost = 0.039, max_diff = -inf (625.6 sec.)\n",
      "[sgd] 1100, cumul. cost = 0.039, max_diff = -inf (712.2 sec.)\n",
      "[sgd] 1200, cumul. cost = 0.041, max_diff = -inf (650.9 sec.)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-467860a5a53b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcur_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# performing SGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mtc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_neg_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m            \u001b[0;31m# performing fixed-point iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdwe_update_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_neg_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_beta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bkkang/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bkkang/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    987\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    988\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bkkang/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Begin training code\n",
    "print('Training...')\n",
    "indexes = range(true_nw)\n",
    "num_epoch = 10       # total number of epochs to train\n",
    "cur_ep = 0\n",
    "cur_lr = 5e-2\n",
    "num_consec_train = 5 # number of consecutive epochs for 'sgd'\n",
    "mode = ['sgd', 'fp']\n",
    "cur_mode = 1 # start with 'fp'. Set this to 0 if you want to start with SGD\n",
    "tol = 1e-3\n",
    "next_schedule = 5\n",
    "dwe_up_cnt = 0\n",
    "beta = 0.7\n",
    "tic = 100\n",
    "\n",
    "while cur_ep < num_epoch:\n",
    "    nr.shuffle(indexes)\n",
    "    neg_ind = indexes[bs:] + indexes[:bs]\n",
    "    cost = 0\n",
    "    totTime = 0\n",
    "    max_diff = -np.inf\n",
    "    cur_beta = beta ** (dwe_up_cnt + 1)\n",
    "    for cur_iter in np.arange(0, nw, bs):\n",
    "        cur_ind = indexes[cur_iter : min(cur_iter + bs, true_nw - 1)]\n",
    "        cur_neg_ind = neg_ind[cur_iter : min(cur_iter + bs, true_nw - 1)]\n",
    "        st = time.time()\n",
    "        if cur_mode == 0: # performing SGD\n",
    "            tc = train_step(cur_ind, cur_neg_ind, cur_lr)\n",
    "        else:            # performing fixed-point iteration\n",
    "            ret = dwe_update_step(cur_ind, cur_neg_ind, cur_beta)\n",
    "            tc, diff = ret[0], ret[1]\n",
    "            max_diff = max(max_diff, float(diff))\n",
    "            dwe_up_cnt += 1\n",
    "        et = time.time()\n",
    "        totTime += (et - st)\n",
    "        cost += tc\n",
    "        if (cur_iter // bs) % tic == 0: # print the *CUMULATIVE* cost\n",
    "            print('[{0}] {1}, cumul. cost = {2:.3f}, max_diff = {3:.3f} ({4:.1f} sec.)'.format(\\\n",
    "                mode[cur_mode], cur_iter // bs, cost, max_diff, totTime))\n",
    "            totTime = 0\n",
    "    if cur_mode == 1 and max_diff < tol:\n",
    "        cur_mode = 0  # switch to SGD\n",
    "        max_diff = -np.inf\n",
    "        next_schedule = cur_ep + num_consec_train\n",
    "    elif cur_mode == 0 and next_schedule == cur_ep:\n",
    "        cur_mode = 1 # switch to fixed-point iteration\n",
    "        dwe_up_cnt = 0\n",
    "        \n",
    "    print('\\t*** Epoch {}, cost = {} ***'.format(cur_ep, cost))\n",
    "    save_model(params, os.path.join(model_out, save_name))\n",
    "    cur_ep += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had to interrupt the training process since it takes several hours before it converges, but you can see that the per-epoch cumulative costs show a decreasing trend.  \n",
    "In the next section, we show how to load this saved model file in order to use the learned embeddings. Of course, we'll use one of the pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model\n",
    "-------------------\n",
    "Next, we'll show you how to *use* the trained model. Since training the above model can take up to several days, we have provided a pre-trained embedding. If you've followed the instructions in the introduction, the path to the pre-trained model file will be stored in variable `saved_model`.\n",
    "\n",
    "The first example we show is a simple nearest neighbor retrieval. For each sample words, we retrieve three of its nearest neighbors to see how close in semantics they are. There can be many notions of \"nearest\", but we again use the cosine similarity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank.n.01\t[(0.38072157, 'sloping.s.02'), (0.30345541, 'acclivitous.s.01'), (0.29497349, 'slop.v.03')]\n",
      "bank.n.05\t[(0.47069725, 'store.n.02'), (0.46000367, 'forehanded.s.02'), (0.45024586, 'hereafter.r.02')]\n",
      "paper.n.01\t[(0.47097549, 'cellulose.n.01'), (0.44648162, 'cellulosic.n.01'), (0.41017959, 'cellulose_xanthate.n.01')]\n",
      "paper.n.05\t[(0.54464161, 'scholarly.a.01'), (0.53724164, 'unscholarly.a.01'), (0.53698415, 'tome.n.01')]\n",
      "apple.n.01\t[(0.56428581, 'cortland.n.01'), (0.53956342, 'winesap.n.01'), (0.50557792, 'delicious.n.01')]\n",
      "boat.n.01\t[(0.42228055, 'regatta.n.01'), (0.42193556, 'mailboat.n.01'), (0.40844288, 'vessel.n.03')]\n"
     ]
    }
   ],
   "source": [
    "def sim(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "with open(saved_model, 'rb') as f:\n",
    "    m = pk.load(f)\n",
    "wd_emb = m['dwe'].get_value() # need to \"download\" the data back to CPU\n",
    "d2i = d['dw2id']\n",
    "i2d = d['id2dw']\n",
    "ind = [d2i['bank.n.01'], d2i['bank.n.05'], d2i['paper.n.01'], d2i['paper.n.05'], d2i['apple.n.01'], d2i['boat.n.01']]\n",
    "\n",
    "for wi in ind:\n",
    "    lst = []\n",
    "    for (wd, _) in enumerate(wd_emb):\n",
    "        if wi == wd: continue\n",
    "        s = sim(wd_emb[wi], wd_emb[wd])\n",
    "        lst.append((s, i2d[wd]))\n",
    "    lst.sort(key = lambda e: e[0], reverse = True) # soft with descending similarity\n",
    "    nn_w = lst[0:3]\n",
    "    print(i2d[wi] + '\\t' + str(nn_w))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the results include apparent false-positives, such as `slop.v.03`. This is because our approach depends largely on the quality of the definitions provided by the dictionary. That is, the richer the description for the word is, the more accurate the embeddings will be. Our setting used a simple WordNet dictionary, which provides short-length definitions that could potentially cause more ambiguities. Nevertheless, some of the nearest neighbors well-reflect the true semantics of the target word. Perhaps you could find a better dataset to train on, in order to refine our results.\n",
    "\n",
    "Next, we'll see how to plot some of the select vectors onto the 2D plane. The dimensionality reduction technique that downsizes the 300 dimensional word vectors is t-SNE. The Python package `scikit-learn` provides a good implementation, so we'll use that. To see different testing settings, try uncommenting one of lines 4~6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQJKEYSBJwjCQJAH/D68A/KgqgUu2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3882a7b7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# words = ['male.n.01', 'king.n.01', 'son.n.01', 'daughter.n.01', 'female.n.01', 'queen.n.02'] # male-female relationship\n",
    "# words = ['drink.n.03', 'drink.v.01', 'eat.v.01', 'magma.n.01', 'rock.n.01', 'food.n.01'] # liquid-solid relationship\n",
    "words = ['man.n.01', 'woman.n.01', 'fireman.n.04', 'housewife.n.01', 'dressmaker.n.01', 'policeman.n.01'] # gender inequality plot\n",
    "tsne = TSNE(n_components = 2, init = 'pca', method = 'exact')\n",
    "X = np.array([wd_emb[d2i[x]] for x in words])\n",
    "Y = tsne.fit_transform(X)\n",
    "\n",
    "plt.scatter(Y[:, 0], Y[:, 1], s = 60, c = ['r', 'g', 'b', 'y', 'c', 'm', 'k', 'w'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
